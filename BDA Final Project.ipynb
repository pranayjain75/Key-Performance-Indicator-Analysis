{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = r'C:\\Users\\Nisarg Shah\\Downloads\\abs1Text\\Text'\n",
    "os.chdir(path)\n",
    "text = []\n",
    "data = []\n",
    "for filename in os.listdir('.'):\n",
    "    with open(filename,encoding=\"utf8\") as myfile:\n",
    "        data.append(myfile.read())\n",
    "    text.append(data)\n",
    "    \n",
    "NO_DOCUMENTS = len(text)\n",
    "print(NO_DOCUMENTS)\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim import models, corpora\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "NUM_TOPICS = 10\n",
    "STOPWORDS = stopwords.words('english')\n",
    " \n",
    "def clean_text(text):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "    return cleaned_text\n",
    " \n",
    "# For gensim we need to tokenize the data and filter out stopwords\n",
    "tokenized_data = []\n",
    "for text in data:\n",
    "    tokenized_data.append(clean_text(text))\n",
    "\n",
    "# Build a Dictionary - association word to numeric id\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    " \n",
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model:\n",
      "Topic #0: 0.013*\"performance\" + 0.011*\"management\" + 0.006*\"practices\" + 0.006*\"journal\" + 0.005*\"research\" + 0.005*\"environmental\" + 0.005*\"study\" + 0.004*\"social\" + 0.004*\"organizational\" + 0.004*\"also\"\n",
      "Topic #1: 0.014*\"performance\" + 0.010*\"management\" + 0.006*\"journal\" + 0.006*\"human\" + 0.005*\"practices\" + 0.005*\"research\" + 0.005*\"study\" + 0.004*\"resource\" + 0.004*\"organizational\" + 0.004*\"social\"\n",
      "Topic #2: 0.011*\"performance\" + 0.009*\"management\" + 0.006*\"environmental\" + 0.005*\"corporate\" + 0.004*\"human\" + 0.004*\"social\" + 0.004*\"study\" + 0.004*\"journal\" + 0.004*\"practices\" + 0.004*\"research\"\n",
      "Topic #3: 0.013*\"performance\" + 0.009*\"management\" + 0.006*\"journal\" + 0.005*\"employees\" + 0.005*\"turnover\" + 0.004*\"may\" + 0.004*\"practices\" + 0.004*\"study\" + 0.004*\"business\" + 0.004*\"human\"\n",
      "Topic #4: 0.014*\"performance\" + 0.009*\"management\" + 0.006*\"practices\" + 0.006*\"organizational\" + 0.005*\"journal\" + 0.005*\"environmental\" + 0.004*\"human\" + 0.004*\"firm\" + 0.004*\"work\" + 0.004*\"business\"\n",
      "Topic #5: 0.012*\"performance\" + 0.007*\"management\" + 0.006*\"journal\" + 0.005*\"social\" + 0.005*\"corporate\" + 0.005*\"firms\" + 0.005*\"firm\" + 0.004*\"research\" + 0.004*\"may\" + 0.004*\"organizational\"\n",
      "Topic #6: 0.011*\"performance\" + 0.008*\"management\" + 0.005*\"environmental\" + 0.005*\"firm\" + 0.004*\"research\" + 0.004*\"practices\" + 0.004*\"journal\" + 0.004*\"social\" + 0.004*\"study\" + 0.004*\"corporate\"\n",
      "Topic #7: 0.013*\"performance\" + 0.006*\"management\" + 0.005*\"journal\" + 0.004*\"may\" + 0.004*\"social\" + 0.004*\"research\" + 0.004*\"relationship\" + 0.004*\"business\" + 0.003*\"firm\" + 0.003*\"environmental\"\n",
      "Topic #8: 0.009*\"performance\" + 0.008*\"management\" + 0.006*\"journal\" + 0.005*\"csr\" + 0.005*\"environmental\" + 0.004*\"social\" + 0.004*\"firm\" + 0.004*\"research\" + 0.004*\"corporate\" + 0.004*\"study\"\n",
      "Topic #9: 0.008*\"performance\" + 0.008*\"environmental\" + 0.008*\"management\" + 0.005*\"social\" + 0.005*\"journal\" + 0.005*\"corporate\" + 0.005*\"firm\" + 0.004*\"may\" + 0.004*\"research\" + 0.004*\"csr\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "print(\"LDA Model:\")\n",
    " \n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 10))\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "datafiles = load_files(r\"C:\\Users\\Nisarg Shah\\Desktop\\abs1Text\\abs1Text - Copy\\Text\")\n",
    "\n",
    "X,y = datafiles.data, datafiles.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):  \n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.corpus import stopwords\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "X = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "tfidfconverter = TfidfTransformer()  \n",
    "X = tfidfconverter.fit_transform(X).toarray()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)  \n",
    "classifier.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  5]\n",
      " [ 4  8]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.76      0.78        21\n",
      "          1       0.62      0.67      0.64        12\n",
      "\n",
      "avg / total       0.73      0.73      0.73        33\n",
      "\n",
      "0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
